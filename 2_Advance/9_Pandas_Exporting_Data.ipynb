{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29facdb",
   "metadata": {},
   "source": [
    "## Exporting Data to csv and exel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c01178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset(\"lukebarousse/data_jobs\")\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Data Clenup\n",
    "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd403e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['job_posted_month'] = df['job_posted_date'].dt.strftime('%b')\n",
    "months = df['job_posted_month'].unique()\n",
    "\n",
    "dict_months = {month: df[df['job_posted_month'] == month] for month in months}\n",
    "\n",
    "# Example: Concatenating January, February, and March data\n",
    "df_q1 = pd.concat([dict_months['Jan'], dict_months['Feb'], dict_months['Mar']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984c8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.to_csv('quarter_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8b267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.to_excel('quarter_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fdaf3",
   "metadata": {},
   "source": [
    "### to_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install sqlalchemy and psycopg2 to use to_sql()\n",
    "# df_q1.to_sql('quarter_1', con=engine, if_exists='replace', index=False)\n",
    "# to_sql() is used to write records stored in a DataFrame to a SQL database.\n",
    "# It requires a SQLAlchemy engine object to connect to the database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a47d12",
   "metadata": {},
   "source": [
    "### to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.to_parquet('quarter_1.parquet', index=False)\n",
    "# to_parquet() is used to write DataFrame to the Parquet format, which is a columnar storage file format.\n",
    "# for datasets that are too lartge to fit into memory, you can use the `chunksize` parameter to write the DataFrame in smaller chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa9ed9",
   "metadata": {},
   "source": [
    "### to_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.to_pickle('quarter_1.pkl')\n",
    "# to_pickle() is used to serialize the DataFrame to a binary format using Python's pickle module.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
